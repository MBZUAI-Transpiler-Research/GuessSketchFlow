{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART model: celinelee/bartlarge_risctoarm_cloze2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <2BD1B165-EC09-3F68-BCE4-8FE4E70CA7E2> /opt/homebrew/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <2A8DB508-8AAF-3FF1-BDFE-9EF17CC2B482> /opt/homebrew/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-11 06:20:08,268 - BartLargeModel - INFO - Initialized BartLargeModel on device: mps\n",
      "2025-04-11 06:20:08,271 - src.helpers.dataset - INFO - Loaded 11 instances\n",
      "2025-04-11 06:20:08,272 - src.helpers.data_loader - INFO - Loaded 11 samples from dataset\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source JSONL: data/processed/RISCV/UnixCommands_risc.jsonl\n",
      "Target JSONL: data/processed/ARM64/UnixCommands_arm.jsonl\n",
      "Loading file: data/processed/RISCV/UnixCommands_risc.jsonl\n",
      "Loaded 11 entries from data/processed/RISCV/UnixCommands_risc.jsonl\n",
      "Loading file: data/processed/ARM64/UnixCommands_arm.jsonl\n",
      "Loaded 11 entries from data/processed/ARM64/UnixCommands_arm.jsonl\n",
      "Source entries: 11\n",
      "Target entries: 11\n",
      "Creating instance 0 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 1 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 2 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 3 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 4 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 5 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 6 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 7 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 8 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 9 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Creating instance 10 with source key risc and target key arm\n",
      "Source entry keys: dict_keys(['source', 'risc', 'risc_output', 'risc_verbose'])\n",
      "Target entry keys: dict_keys(['source', 'arm', 'arm_output', 'arm_verbose'])\n",
      "Created 11 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2189 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from src.guess.guess import ConfigType, Guess, Config\n",
    "from src.domain.datasets.UnixCommandDataset import UnixCommandDataset  # Register dataset\n",
    "from src.domain.datasets.ProjectEulerDataset import ProjectEulerDataset\n",
    "\n",
    "config = Config(ConfigType.BART_RISC2ARM.get_path())\n",
    "guess = Guess(config=config)\n",
    "predictions = guess.guess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sketch.sketch import Sketch\n",
    "\n",
    "sketch = Sketch(config, guess.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def html_escape(text):\n",
    "    # Convert special characters to HTML-safe versions\n",
    "    return (text.replace(\"&\", \"&amp;\")\n",
    "                .replace(\"<\", \"&lt;\")\n",
    "                .replace(\">\", \"&gt;\")\n",
    "                .replace(\"\\n\", \"<br>\")\n",
    "                .replace(\"\\t\", \"&nbsp;&nbsp;&nbsp;&nbsp;\"))  # 4 spaces for a tab\n",
    "\n",
    "def display_blocks(blocks, pred):\n",
    "    data = []\n",
    "\n",
    "    for block in blocks:\n",
    "        source_text = sketch.model.tokenizer.decode(\n",
    "            pred.source[0][block.source_start:block.source_end]\n",
    "        )\n",
    "        pred_text = sketch.model.tokenizer.decode(\n",
    "            pred.pred[0][block.pred_start:block.pred_end]\n",
    "        )\n",
    "        data.append({\n",
    "            'SOURCE': html_escape(source_text),\n",
    "            'PRED': html_escape(pred_text)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    styles = \"\"\"\n",
    "    <style>\n",
    "    table {\n",
    "        table-layout: auto;\n",
    "        word-wrap: break-word;\n",
    "    }\n",
    "    td {\n",
    "        white-space: normal !important;\n",
    "        font-family: monospace;\n",
    "        vertical-align: top;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(styles + df.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnixCommands/cat\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.2857142857142857\n",
      "UnixCommands/cd\n",
      "Ratio of invalid_blocks: 0.1111111111111111\n",
      "Ratio of non_equivalent_blocks: 0.3333333333333333\n",
      "UnixCommands/cp\n",
      "Ratio of invalid_blocks: 0.3125\n",
      "Ratio of non_equivalent_blocks: 0.5625\n",
      "UnixCommands/ls\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.20833333333333334\n",
      "UnixCommands/mkdir\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.0\n",
      "UnixCommands/ps\n",
      "Ratio of invalid_blocks: 0.2\n",
      "Ratio of non_equivalent_blocks: 0.6\n",
      "UnixCommands/rm\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.0\n",
      "UnixCommands/rmdir\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.0\n",
      "UnixCommands/tee\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.42857142857142855\n",
      "UnixCommands/touch\n",
      "Ratio of invalid_blocks: 0.0\n",
      "Ratio of non_equivalent_blocks: 0.0\n",
      "UnixCommands/xargs\n",
      "Ratio of invalid_blocks: 0.25\n",
      "Ratio of non_equivalent_blocks: 0.5\n"
     ]
    }
   ],
   "source": [
    "results = sketch.sketch(predictions)\n",
    "\n",
    "for pred_result, sketch_result in results:\n",
    "    print(pred_result.instance_id)\n",
    "    print(f\"Ratio of invalid_blocks: {len(sketch_result.invalid_blocks)/len(sketch_result.total_blocks)}\")\n",
    "    print(f\"Ratio of non_equivalent_blocks: {len(sketch_result.non_equivalent_blocks)/len(sketch_result.total_blocks)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
